# metropet-backend/app/llm/orchestrator.py

import os
import json
from openai import OpenAI, AsyncOpenAI
from dotenv import load_dotenv
from typing import List, Dict, AsyncGenerator

from .tools import FUNCTION_MAP

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# --- LLM Client è¨­å®š ---
LLM_MODE = os.getenv("LLM_MODE", "openai")
MODEL_NAME = os.getenv("MODEL_NAME", "gpt-4o-mini")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "breeze-7b-instruct-v1_0:latest")
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434/v1")

# --- æ ¹æ“š LLM_MODE åˆå§‹åŒ– AsyncOpenAI Client (æ¨è–¦ä½¿ç”¨ç•°æ­¥ç‰ˆæœ¬) ---
if LLM_MODE == 'local':
    print(f"ğŸš€ Running in LOCAL mode with Ollama model: {OLLAMA_MODEL}")
    client = AsyncOpenAI(
        base_url=OLLAMA_BASE_URL,
        api_key="ollama",  # Ollama API é‡‘é‘°å›ºå®šç‚º 'ollama'
    )
    model_to_use = OLLAMA_MODEL
else:
    print(f"â˜ï¸ Running in OPENAI mode with model: {MODEL_NAME}")
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OPENAI_API_KEY is not set in the .env file")
    client = AsyncOpenAI(api_key=api_key)
    model_to_use = MODEL_NAME


async def chat_stream(messages: List[Dict], user_id: str) -> AsyncGenerator[str, None]:
    """
    çµ±ä¸€çš„æµå¼èŠå¤©è™•ç†ï¼Œæ”¯æ´ Tool Callingã€‚
    """
    try:
        # --- æ­¥é©Ÿ 1: åˆæ¬¡å‘¼å« LLMï¼Œåˆ¤æ–·æ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…· ---
        # æ³¨æ„ï¼šå°æ–¼æœ¬åœ°æ¨¡å‹ï¼Œä¸¦éæ‰€æœ‰æ¨¡å‹éƒ½å®Œç¾æ”¯æ´ OpenAI çš„ tools æ ¼å¼ï¼Œ
        # Breeze å’Œ Llama 3.1 æ”¯æ´åº¦è¼ƒå¥½ã€‚
        tools = [{"type": "function", "function": f.model_json_schema()} for f, _ in FUNCTION_MAP.values()]
        
        first_response = await client.chat.completions.create(
            model=model_to_use,
            messages=messages,
            tools=tools,
            tool_choice="auto",
        )
        response_message = first_response.choices[0].message

        # --- æ­¥é©Ÿ 2: æª¢æŸ¥ LLM æ˜¯å¦è¦æ±‚åŸ·è¡Œå·¥å…· ---
        tool_calls = response_message.tool_calls
        if not tool_calls:
            # å¦‚æœä¸éœ€è¦å·¥å…·ï¼Œç›´æ¥ä»¥æµå¼æ–¹å¼å›å‚³æœ€çµ‚ç­”æ¡ˆ
            stream = await client.chat.completions.create(
                model=model_to_use,
                messages=messages + [response_message],
                stream=True
            )
            async for chunk in stream:
                content = chunk.choices[0].delta.content
                if content:
                    yield content
            return

        # --- æ­¥é©Ÿ 3: å¦‚æœéœ€è¦ï¼ŒåŸ·è¡Œæ‰€æœ‰å·¥å…· ---
        print(f"ğŸ› ï¸ Executing tools: {[call.function.name for call in tool_calls]}")
        tool_outputs = []
        for call in tool_calls:
            fn_name = call.function.name
            args = json.loads(call.function.arguments)
            
            model_cls, func = FUNCTION_MAP[fn_name]
            validated_args = model_cls(**args)
            result = await func(validated_args)
            
            tool_outputs.append({
                "tool_call_id": call.id,
                "role": "tool",
                "name": fn_name,
                "content": json.dumps(result, ensure_ascii=False),
            })

        # --- æ­¥é©Ÿ 4: å°‡å·¥å…·åŸ·è¡Œçµæœå›å‚³çµ¦ LLMï¼Œä»¥æµå¼æ–¹å¼ç”Ÿæˆæœ€çµ‚å›ç­” ---
        messages.append(response_message)
        messages.extend(tool_outputs)

        final_stream = await client.chat.completions.create(
            model=model_to_use,
            messages=messages,
            stream=True
        )
        async for chunk in final_stream:
            content = chunk.choices[0].delta.content
            if content:
                yield content

    except Exception as e:
        print(f"An error occurred during chat stream: {e}")
        error_message = f"è™•ç†è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"
        yield error_message